{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6OO25eppgQv"
   },
   "source": [
    "### Loading libraries and Data Using Kaggle Api "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NC2NsJ0VVQfJ",
    "outputId": "4645e08e-525f-4004-d5d4-be9b0584e20c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1MjCMLaEalh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "y8neqEYxFv4Q",
    "outputId": "3a6e54c8-0c39-4e66-d838-b5b1d4c9481c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-b5a1ccd7-03b1-4a30-914e-68dbc5c74e58\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-b5a1ccd7-03b1-4a30-914e-68dbc5c74e58\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving kaggle.json to kaggle (3).json\n",
      "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.10 / client 1.5.4)\n",
      "merchants.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
      "sample_submission.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
      "train.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
      "Data%20Dictionary.xlsx: Skipping, found more recently modified local copy (use --force to force download)\n",
      "Data_Dictionary.xlsx: Skipping, found more recently modified local copy (use --force to force download)\n",
      "historical_transactions.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
      "test.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
      "new_merchant_transactions.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "! pip install -q kaggle\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "files.upload()\n",
    "\n",
    "! mkdir ~/.kaggle\n",
    "\n",
    "! cp kaggle.json ~/.kaggle/\n",
    "\n",
    "! chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "! kaggle competitions download -c elo-merchant-category-recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rueQvxVdEqjl",
    "outputId": "315726c7-d14c-49f0-bfbc-f19fab853cf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /content/historical_transactions.csv.zip\n",
      "replace historical_transactions.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: Archive:  /content/merchants.csv.zip\n",
      "replace merchants.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: Archive:  /content/new_merchant_transactions.csv.zip\n",
      "replace new_merchant_transactions.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: Archive:  /content/train.csv.zip\n",
      "replace train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: Archive:  /content/test.csv.zip\n",
      "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "# unzipping the dataset\n",
    "! unzip '/content/historical_transactions.csv.zip'\n",
    "\n",
    "! unzip '/content/merchants.csv.zip'\n",
    "\n",
    "! unzip '/content/new_merchant_transactions.csv.zip'\n",
    "\n",
    "! unzip '/content/train.csv.zip'\n",
    "\n",
    "! unzip '/content/test.csv.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7RFkiliLFg6a"
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/champs-scalar-coupling/discussion/96655\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    #print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nRKd2pXmMxSw",
    "outputId": "de6ec7fc-1c17-4c65-bb60-f229076c042c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 2.24 MB\n"
     ]
    }
   ],
   "source": [
    "test = reduce_mem_usage(pd.read_csv('/content/test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dXnTK-1vubz1"
   },
   "outputs": [],
   "source": [
    "def train_features(train,test):\n",
    "\n",
    "  # imputing missing values with mode\n",
    "  # https://stackoverflow.com/questions/42789324/pandas-fillna-mode\n",
    "  test['first_active_month'].fillna(test['first_active_month'].mode()[0], inplace=True)\n",
    "\n",
    "  # converting date features to datetime\n",
    "  train['first_active_month']=pd.to_datetime(train['first_active_month'])\n",
    "  test['first_active_month']=pd.to_datetime(test['first_active_month'])\n",
    "\n",
    "  # making a new columns with outliers as seen in eda notebook\n",
    "  train['outliers'] = 0\n",
    "  train.loc[train['target'] < -30, 'outliers'] = 1\n",
    "\n",
    "  # https://www.geeksforgeeks.org/mean-encoding-machine-learning/\n",
    "  # mean encoding categorical features by grouping them with outliers.\n",
    "  for feature in ['feature_1', 'feature_2', 'feature_3']:\n",
    "    mapping = train.groupby([feature])['outliers'].mean()\n",
    "    train[feature] = train[feature].map(mapping)\n",
    "    test[feature] = test[feature].map(mapping)\n",
    "\n",
    "  # https://www.kaggle.com/mks2192/feature-engineering\n",
    "  train['quarter']=train['first_active_month'].dt.quarter\n",
    "  train['total_time'] = (datetime.datetime.today() - train['first_active_month']).dt.days\n",
    "  train['start_month'] = train['first_active_month'].dt.month\n",
    "  train['start_year'] = train['first_active_month'].dt.year\n",
    "  train['dayofweek'] = train['first_active_month'].dt.dayofweek\n",
    "  train['quarter']=train['first_active_month'].dt.quarter\n",
    "\n",
    "  train['total_time_feature1']=train['total_time']*train['feature_1']\n",
    "  train['total_time_feature2']=train['total_time']*train['feature_2']\n",
    "  train['total_time_feature3']=train['total_time']*train['feature_3']\n",
    "\n",
    "  train['total_time_feature1_ratio']=(train['feature_1']/train['total_time'])\n",
    "  train['total_time_feature2_ratio']=(train['feature_2']/train['total_time'])\n",
    "  train['total_time_feature3_ratio']=(train['feature_3']/train['total_time'])\n",
    "\n",
    "  # getting aggregated features from categorical variables\n",
    "  train['feature_sum'] = train['feature_1'] + train['feature_2'] + train['feature_3']\n",
    "  train['feature_mean'] = train['feature_sum']/3\n",
    "  train['feature_max'] = train[['feature_1', 'feature_2', 'feature_3']].max(axis=1)\n",
    "  train['feature_min'] = train[['feature_1', 'feature_2', 'feature_3']].min(axis=1)\n",
    "  train['feature_var'] = train[['feature_1', 'feature_2', 'feature_3']].std(axis=1)\n",
    "\n",
    "  # https://www.kaggle.com/mks2192/feature-engineering\n",
    "  test['quarter']=test['first_active_month'].dt.quarter\n",
    "  test['total_time'] = (datetime.datetime.today() - test['first_active_month']).dt.days\n",
    "  test['start_month'] = test['first_active_month'].dt.month\n",
    "  test['start_year'] = test['first_active_month'].dt.year\n",
    "  test['dayofweek'] = test['first_active_month'].dt.dayofweek\n",
    "  test['quarter']=test['first_active_month'].dt.quarter\n",
    "\n",
    "  test['total_time_feature1']=test['total_time']*test['feature_1']\n",
    "  test['total_time_feature2']=test['total_time']*test['feature_2']\n",
    "  test['total_time_feature3']=test['total_time']*test['feature_3']\n",
    "\n",
    "  test['total_time_feature1_ratio']=(test['feature_1']/test['total_time'])\n",
    "  test['total_time_feature2_ratio']=(test['feature_2']/test['total_time'])\n",
    "  test['total_time_feature3_ratio']=(test['feature_3']/test['total_time'])\n",
    "\n",
    "  # getting aggregated features from categorical variables\n",
    "  test['feature_sum'] = test['feature_1'] + test['feature_2'] + test['feature_3']\n",
    "  test['feature_mean'] = test['feature_sum']/3\n",
    "  test['feature_max'] = test[['feature_1', 'feature_2', 'feature_3']].max(axis=1)\n",
    "  test['feature_min'] = test[['feature_1', 'feature_2', 'feature_3']].min(axis=1)\n",
    "  test['feature_var'] = test[['feature_1', 'feature_2', 'feature_3']].std(axis=1)\n",
    "\n",
    "  gc.collect()\n",
    "\n",
    "  return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7oZPY5PmlO_T"
   },
   "outputs": [],
   "source": [
    "def hist_features(hist_trans):\n",
    "\n",
    "  # preprocessing the csv file\n",
    "  # imputing the missing values\n",
    "  hist_trans['category_3'].fillna(hist_trans['category_3'].mode()[0], inplace=True)\n",
    "  hist_trans['merchant_id'].fillna(hist_trans['merchant_id'].mode()[0], inplace=True)\n",
    "  hist_trans['category_2'].fillna(hist_trans['category_2'].mode()[0], inplace=True)\n",
    "\n",
    "  # mapping catrgorical variables\n",
    "  hist_trans['authorized_flag'] = hist_trans['authorized_flag'].map({'Y':1, 'N':0})\n",
    "  hist_trans['category_1'] = hist_trans['category_1'].map({'Y':1, 'N':0})\n",
    "  hist_trans['category_3'] = hist_trans['category_3'].map({'A':0, 'B':1, 'C':2})\n",
    "  hist_trans['installments'] = hist_trans['installments'].map({-1:13, 0:0.1,1:1,2:2,3:3,4:4,5:5,6:6,7:7,8:8,9:9,10:10,11:11,12:12,999:13})\n",
    "\n",
    "  # taking 99 percrntile value a max to remove the outliers\n",
    "  hist_trans['purchase_amount'] = hist_trans['purchase_amount'].apply(lambda x: min(x, 1.22))\n",
    "\n",
    "  # feature engineering based on dates\n",
    "  hist_trans['purchase_date']=pd.to_datetime(hist_trans['purchase_date'])\n",
    "  hist_trans['year'] = hist_trans['purchase_date'].dt.year\n",
    "  hist_trans['day'] = hist_trans['purchase_date'].dt.day\n",
    "  hist_trans['month'] = hist_trans['purchase_date'].dt.month\n",
    "  hist_trans['dayofweek'] = hist_trans['purchase_date'].dt.dayofweek\n",
    "  hist_trans['weekofyear'] = hist_trans['purchase_date'].dt.weekofyear\n",
    "  hist_trans['hour_of_purchase'] = hist_trans['purchase_date'].dt.hour\n",
    "  hist_trans['Minute_of_purchase'] = hist_trans['purchase_date'].dt.minute\n",
    "  hist_trans['Second_of_purchase'] = hist_trans['purchase_date'].dt.second\n",
    "  hist_trans['purchased_on_weekend'] = (hist_trans.dayofweek >=5).astype(int)\n",
    "  hist_trans['purchased_on_weekday'] = (hist_trans.dayofweek <5).astype(int)\n",
    "\n",
    "  hist_trans['month_diff'] = ((datetime.datetime.today() - hist_trans['purchase_date']).dt.days)//30\n",
    "  hist_trans['month_diff'] += hist_trans['month_lag']\n",
    "\n",
    "  # feature engineering based on installments and purchase amount\n",
    "  # purchase amount is highly normalized so we denormalizing it  \n",
    "  # inspired from https://chandureddyvari.com/posts/elo-merchant-feature/\n",
    "  hist_trans['EMI'] = hist_trans['purchase_amount'] / hist_trans['installments']\n",
    "  hist_trans['purchase_amount_quantiles'] = pd.qcut(hist_trans['purchase_amount'], 5, labels=False)\n",
    "  hist_trans['duration'] = hist_trans['purchase_amount']*hist_trans['month_diff']\n",
    "  hist_trans['amount_month_ratio'] = hist_trans['purchase_amount']/hist_trans['month_diff']\n",
    "\n",
    "  hist_trans = reduce_mem_usage(hist_trans)\n",
    "\n",
    "  # aggregating by grouping them by card_id.\n",
    "  aggregations = {\n",
    "    'purchase_date' : ['max','min'],\n",
    "    'purchased_on_weekend': ['sum', 'mean'],\n",
    "    'purchased_on_weekday': ['sum', 'mean'],\n",
    "    'dayofweek' : ['nunique', 'sum', 'mean','max'],\n",
    "    'hour_of_purchase': ['nunique', 'mean', 'min', 'max'],\n",
    "    'Minute_of_purchase': ['nunique', 'mean', 'min', 'max'],\n",
    "    'Second_of_purchase': ['nunique', 'mean', 'min', 'max'],\n",
    "    'weekofyear': ['nunique', 'mean', 'min', 'max'],\n",
    "    'month_diff': ['max','min','mean','var','skew'],\n",
    "    'day': ['nunique', 'sum', 'min'],\n",
    "    'month' : ['sum', 'mean', 'nunique','max'],\n",
    "    'purchase_amount_quantiles' : ['var', 'mean', 'skew'],\n",
    "    'duration' : ['mean','min','max','var','skew'],\n",
    "    'amount_month_ratio' : ['mean','min','max','var','skew'],\n",
    "    'authorized_flag' : ['sum','mean'],\n",
    "    'subsector_id': ['nunique'],\n",
    "    'card_id': ['size'],\n",
    "    'city_id' : ['nunique'],\n",
    "    'state_id' : ['nunique'],\n",
    "    'merchant_id': ['nunique'],\n",
    "    'installments': ['sum','max','mean','var','skew'],\n",
    "    'merchant_category_id': ['nunique'],\n",
    "    'purchase_amount': ['sum', 'mean', 'min', 'max', 'var','skew'],\n",
    "    'EMI' : ['sum','mean','max','min','var'],\n",
    "    'category_1' : ['sum','mean', 'max','min'],\n",
    "    'category_2' : ['sum','mean'],\n",
    "    'category_3' : ['sum','mean'],\n",
    "    'month_lag' : ['sum','max','min','mean','var','skew']\n",
    "  }\n",
    "  aggregated_trans = hist_trans.groupby('card_id').agg(aggregations)\n",
    "  aggregated_trans.columns = ['transactions_'+'_'.join(col).strip()\n",
    "                           for col in aggregated_trans.columns.values]\n",
    "  aggregated_trans.reset_index(inplace=True)\n",
    "\n",
    "  # extracting some more features based on aggregated features.\n",
    "  aggregated_trans['transactions_purchase_date_diff'] = (aggregated_trans['transactions_purchase_date_max']-aggregated_trans['transactions_purchase_date_min']).dt.days\n",
    "  aggregated_trans['transactions_purchase_date_average'] = aggregated_trans['transactions_purchase_date_diff']/aggregated_trans['transactions_card_id_size']\n",
    "  aggregated_trans['transactions_purchase_date_uptonow'] = (datetime.datetime.today()-aggregated_trans['transactions_purchase_date_max']).dt.days\n",
    "  aggregated_trans['transactions_purchase_date_uptomin'] = (datetime.datetime.today()-aggregated_trans['transactions_purchase_date_min']).dt.days\n",
    "\n",
    "  gc.collect()\n",
    "  return aggregated_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d-E7yyY0vzqe"
   },
   "outputs": [],
   "source": [
    "def get_new_trans_features(new_hist_trans):\n",
    "\n",
    "  # preprocessing the csv file\n",
    "  # imputing the missing values\n",
    "  new_hist_trans['category_3'].fillna(new_hist_trans['category_3'].mode()[0], inplace=True)\n",
    "  new_hist_trans['merchant_id'].fillna(new_hist_trans['merchant_id'].mode()[0], inplace=True)\n",
    "  new_hist_trans['category_2'].fillna(new_hist_trans['category_2'].mode()[0], inplace=True)\n",
    "\n",
    "  # mapping catrgorical variables\n",
    "  new_hist_trans['authorized_flag'] = new_hist_trans['authorized_flag'].map({'Y':1, 'N':0})\n",
    "  new_hist_trans['category_1'] = new_hist_trans['category_1'].map({'Y':1, 'N':0})\n",
    "  new_hist_trans['category_3'] = new_hist_trans['category_3'].map({'A':0, 'B':1, 'C':2})\n",
    "  new_hist_trans['installments'] = new_hist_trans['installments'].map({-1:13, 0:0.1,1:1,2:2,3:3,4:4,5:5,6:6,7:7,8:8,9:9,10:10,11:11,12:12,999:13})\n",
    "\n",
    "  # taking 99 percrntile value a max to remove the outliers\n",
    "  new_hist_trans['purchase_amount'] = new_hist_trans['purchase_amount'].apply(lambda x: min(x, 1.22))\n",
    "\n",
    "  # feature engineering based on dates\n",
    "  new_hist_trans['purchase_date']=pd.to_datetime(new_hist_trans['purchase_date'])\n",
    "  new_hist_trans['year'] = new_hist_trans['purchase_date'].dt.year\n",
    "  new_hist_trans['day'] = new_hist_trans['purchase_date'].dt.day\n",
    "  new_hist_trans['month'] = new_hist_trans['purchase_date'].dt.month\n",
    "  new_hist_trans['dayofweek'] = new_hist_trans['purchase_date'].dt.dayofweek\n",
    "  new_hist_trans['weekofyear'] = new_hist_trans['purchase_date'].dt.weekofyear\n",
    "  new_hist_trans['hour_of_purchase'] = new_hist_trans['purchase_date'].dt.hour\n",
    "  new_hist_trans['Minute_of_purchase'] = new_hist_trans['purchase_date'].dt.minute\n",
    "  new_hist_trans['Second_of_purchase'] = new_hist_trans['purchase_date'].dt.second\n",
    "  new_hist_trans['purchased_on_weekend'] = (new_hist_trans.dayofweek >=5).astype(int)\n",
    "  new_hist_trans['purchased_on_weekday'] = (new_hist_trans.dayofweek <5).astype(int)\n",
    "\n",
    "  new_hist_trans['month_diff'] = ((datetime.datetime.today() - new_hist_trans['purchase_date']).dt.days)//30\n",
    "  new_hist_trans['month_diff'] += new_hist_trans['month_lag']\n",
    "\n",
    "  # feature engineering based on installments and purchase amount\n",
    "  # purchase amount is highly normalized so we denormalizing it  \n",
    "  # inspired from https://chandureddyvari.com/posts/elo-merchant-feature/\n",
    "  new_hist_trans['EMI'] = new_hist_trans['purchase_amount'] / new_hist_trans['installments']\n",
    "  new_hist_trans['purchase_amount_quantiles'] = pd.qcut(new_hist_trans['purchase_amount'], 5, labels=False)\n",
    "  new_hist_trans['duration'] = new_hist_trans['purchase_amount']*new_hist_trans['month_diff']\n",
    "  new_hist_trans['amount_month_ratio'] = new_hist_trans['purchase_amount']/new_hist_trans['month_diff']\n",
    "\n",
    "  new_hist_trans = reduce_mem_usage(new_hist_trans)\n",
    "\n",
    "  # aggregating by grouping them by card_id.\n",
    "  aggregations = {\n",
    "    'purchase_date' : ['max','min'],\n",
    "    'purchased_on_weekend': ['sum', 'mean'],\n",
    "    'purchased_on_weekday': ['sum', 'mean'],\n",
    "    'dayofweek' : ['nunique', 'sum', 'mean','max'],\n",
    "    'hour_of_purchase': ['nunique', 'mean', 'min', 'max'],\n",
    "    'Minute_of_purchase': ['nunique', 'mean', 'min', 'max'],\n",
    "    'Second_of_purchase': ['nunique', 'mean', 'min', 'max'],\n",
    "    'weekofyear': ['nunique', 'mean', 'min', 'max'],\n",
    "    'month_diff': ['max','min','mean','var','skew'],\n",
    "    'day': ['nunique', 'sum', 'min'],\n",
    "    'month' : ['sum', 'mean', 'nunique','max'],\n",
    "    'purchase_amount_quantiles' : ['var', 'mean', 'skew'],\n",
    "    'duration' : ['mean','min','max','var','skew'],\n",
    "    'amount_month_ratio' : ['mean','min','max','var','skew'],\n",
    "    'subsector_id': ['nunique'],\n",
    "    'card_id': ['size'],\n",
    "    'city_id' : ['nunique'],\n",
    "    'state_id' : ['nunique'],\n",
    "    'merchant_id': ['nunique'],\n",
    "    'installments': ['sum','max','mean','var','skew'],\n",
    "    'merchant_category_id': ['nunique'],\n",
    "    'purchase_amount': ['sum', 'mean', 'min', 'max', 'var','skew'],\n",
    "    'EMI' : ['sum','mean','max','min','var'],\n",
    "    'category_1' : ['sum','mean', 'max','min'],\n",
    "    'category_2' : ['sum','mean'],\n",
    "    'category_3' : ['sum','mean'],\n",
    "    'month_lag' : ['sum','max','min','mean','var','skew']\n",
    "  }\n",
    "\n",
    "  aggregated_trans_1 = new_hist_trans.groupby('card_id').agg(aggregations)\n",
    "  aggregated_trans_1.columns = ['new_transactions_'+'_'.join(col).strip()\n",
    "                           for col in aggregated_trans_1.columns.values]\n",
    "  aggregated_trans_1.reset_index(inplace=True)\n",
    "\n",
    "  # extracting some more features based on aggregated features.\n",
    "  aggregated_trans_1['new_transactions_purchase_date_diff'] = (aggregated_trans_1['new_transactions_purchase_date_max']-aggregated_trans_1['new_transactions_purchase_date_min']).dt.days\n",
    "  aggregated_trans_1['new_transactions_purchase_date_average'] = aggregated_trans_1['new_transactions_purchase_date_diff']/aggregated_trans_1['new_transactions_card_id_size']\n",
    "  aggregated_trans_1['new_transactions_purchase_date_uptonow'] = (datetime.datetime.today()-aggregated_trans_1['new_transactions_purchase_date_max']).dt.days\n",
    "  aggregated_trans_1['new_transactions_purchase_date_uptomin'] = (datetime.datetime.today()-aggregated_trans_1['new_transactions_purchase_date_min']).dt.days\n",
    "  \n",
    "  return aggregated_trans_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nr8GQ2WUyJ91"
   },
   "outputs": [],
   "source": [
    "def get_train_features(train):\n",
    "  \n",
    "  print('loading data ........')\n",
    "  test = reduce_mem_usage(pd.read_csv('/content/test.csv'))\n",
    "  hist_trans = reduce_mem_usage(pd.read_csv('/content/historical_transactions.csv'))\n",
    "  new_hist_trans = reduce_mem_usage(pd.read_csv('/content/new_merchant_transactions.csv'))\n",
    "  print('processing train......')\n",
    "\n",
    "  train1,test1 = train_features(train,test)\n",
    "  \n",
    "  print('processing historical transactions.....')\n",
    "  aggregated_trans = hist_features(hist_trans)\n",
    "  print('processing new merchant transactions.....')\n",
    "  aggregated_trans_1 = get_new_trans_features(new_hist_trans)\n",
    "  print('merging all files together......')\n",
    "  train1=pd.merge(train1, aggregated_trans, on='card_id', how='left')\n",
    "  train1=pd.merge(train1, aggregated_trans_1, on='card_id', how='left')\n",
    "\n",
    "  print('creating some more new features from merged file......')\n",
    "  # converting engineered date features to datetime so that we can use them afterwards.\n",
    "  train1['transactions_purchase_date_max'] = pd.to_datetime(train1['transactions_purchase_date_max'])\n",
    "  train1['transactions_purchase_date_min'] = pd.to_datetime(train1['transactions_purchase_date_min'])\n",
    "  train1['new_transactions_purchase_date_max'] = pd.to_datetime(train1['new_transactions_purchase_date_max'])\n",
    "  train1['new_transactions_purchase_date_min'] = pd.to_datetime(train1['new_transactions_purchase_date_min'])\n",
    "  \n",
    "  # extracting some more features from train by performing some simple caluculations.\n",
    "  # inspired by https://www.kaggle.com/mfjwr1/simple-lightgbm-without-blending\n",
    "  train1['transactions_purchase_date_difference']=train1['transactions_purchase_date_max'] - train1['transactions_purchase_date_min']\n",
    "  train1['new_transactions_purchase_date_difference'] = train1['new_transactions_purchase_date_max'] - train1['new_transactions_purchase_date_min']\n",
    "  train1['Avg_purchase'] = train1['transactions_purchase_date_difference'] / train1['transactions_card_id_size']\n",
    "  train1['new_Avg_purchase'] = train1['new_transactions_purchase_date_difference'] / train1['new_transactions_card_id_size']\n",
    "  train1['last_purchase_from_now'] = (datetime.datetime.today() - train1['transactions_purchase_date_max']).dt.days\n",
    "  train1['new_last_purchase_from_now'] = (datetime.datetime.today() - train1['new_transactions_purchase_date_max']).dt.days\n",
    "  train1['first_purchase_from_now'] = (datetime.datetime.today() - train1['transactions_purchase_date_min']).dt.days\n",
    "  train1['new_first_purchase_from_now'] = (datetime.datetime.today() - train1['new_transactions_purchase_date_min']).dt.days\n",
    "\n",
    "  train1['card_id_total'] = train1['new_transactions_card_id_size']+train1['transactions_card_id_size']\n",
    "  train1['card_id_ratio'] =  train1['new_transactions_card_id_size']/train1['transactions_card_id_size']\n",
    "\n",
    "  train1['total_purchase_amount_max'] = train1['new_transactions_purchase_amount_max']+train1['transactions_purchase_amount_max']\n",
    "  train1['total_purchase_amount_min'] = train1['new_transactions_purchase_amount_min']+train1['transactions_purchase_amount_min']\n",
    "  train1['total_purchase_amount_mean'] = train1['new_transactions_purchase_amount_mean']+train1['transactions_purchase_amount_mean']\n",
    "  train1['total_purchase_amount_sum'] = train1['new_transactions_purchase_amount_sum']+train1['transactions_purchase_amount_sum']\n",
    "  train1['total_purchase_amount_ratio'] = train1['new_transactions_purchase_amount_sum']/train1['transactions_purchase_amount_sum']\n",
    "\n",
    "  train1['total_installments_max'] = train1['new_transactions_installments_max'] + train1['transactions_installments_max']\n",
    "  train1['total_installments_mean'] = train1['new_transactions_installments_mean'] + train1['transactions_installments_mean']\n",
    "  train1['total_installments_sum'] = train1['new_transactions_installments_sum'] + train1['transactions_installments_sum']\n",
    "  train1['total_installments_ratio'] = train1['new_transactions_installments_sum'] / train1['transactions_installments_sum']\n",
    "\n",
    "  train1['total_month_lag_max'] = train1['new_transactions_month_lag_max'] + train1['transactions_month_lag_max']\n",
    "  train1['total_month_lag_min'] = train1['new_transactions_month_lag_min'] + train1['transactions_month_lag_min']\n",
    "  train1['total_month_lag_mean'] = train1['new_transactions_month_lag_mean'] + train1['transactions_month_lag_mean']\n",
    "  train1['total_month_lag_sum'] = train1['new_transactions_month_lag_sum'] + train1['transactions_month_lag_sum']\n",
    "  train1['total_month_lag_ratio'] = train1['new_transactions_month_lag_sum'] / train1['transactions_month_lag_sum']\n",
    "\n",
    "  train1['total_duration_max'] = train1['new_transactions_duration_max'] + train1['transactions_duration_max']\n",
    "  train1['total_duration_min'] = train1['new_transactions_duration_min'] + train1['transactions_duration_min']\n",
    "  train1['total_duration_mean'] = train1['new_transactions_duration_mean'] + train1['transactions_duration_mean']\n",
    "\n",
    "\n",
    "  train1['total_month_diff_max'] = train1['new_transactions_month_diff_max'] + train1['transactions_month_diff_max']\n",
    "  train1['total_month_diff_mean'] = train1['new_transactions_month_diff_mean'] + train1['transactions_month_diff_mean']\n",
    "  train1['total_month_diff_min'] = train1['new_transactions_month_diff_min'] + train1['transactions_month_diff_min']\n",
    "\n",
    "  train1['total_amount_month_ratio_max'] = train1['new_transactions_amount_month_ratio_max'] + train1['transactions_amount_month_ratio_max']\n",
    "  train1['total_amount_month_ratio_min'] = train1['new_transactions_amount_month_ratio_min'] + train1['transactions_amount_month_ratio_min']\n",
    "  train1['total_amount_month_ratio_mean'] = train1['new_transactions_amount_month_ratio_mean'] + train1['transactions_amount_month_ratio_mean']\n",
    "\n",
    "  train1['customer_rating'] = train1['transactions_card_id_size'] * train1['transactions_purchase_amount_sum'] / train1['transactions_month_diff_mean']\n",
    "  train1['new_customer_rating'] = train1['new_transactions_card_id_size'] * train1['new_transactions_purchase_amount_sum'] / train1['new_transactions_month_diff_mean']\n",
    "  train1['customer_rating_ratio'] = train1['customer_rating'] / train1['new_customer_rating']\n",
    "  \n",
    "  print('preprocessing final data........')\n",
    "  # replacing inf values with nan.\n",
    "  train1.replace([-np.inf,np.inf], np.nan, inplace=True)\n",
    "  \n",
    "  # checking for nan values.\n",
    "  k= train1.columns[train1.isna().any()]\n",
    "\n",
    "  # imputing using mode\n",
    "  for i in range(len(k)):\n",
    "    train1[k[i]].fillna(train1[k[i]].mode()[0], inplace=True)\n",
    "\n",
    "  # getting columns having datetime64[ns] datatypes\n",
    "  types=train1.select_dtypes(include=['datetime64[ns]']).columns\n",
    "\n",
    "  # removing columns having datetime64[ns] datatypes\n",
    "  train1=train1.drop(types,axis=1)\n",
    "\n",
    "  # getting columns having timedelta64[ns] datatypes\n",
    "  types1=train1.select_dtypes(include=['timedelta64[ns]']).columns\n",
    "  \n",
    "  # changing timedelta64[ns] to int64 datatype so that we can perform modelling\n",
    "  for i in types1:\n",
    "    train1[i] = train1[i].astype(np.int64) * 1e-9\n",
    "  \n",
    "  k = [i for i in range(201917)]\n",
    "  train1.insert(loc=0, column='Unnamed', value=k)\n",
    "\n",
    "  train1_cols = [c for c in train1.columns if c not in ['card_id','target','outliers']]\n",
    "\n",
    "  return train1[train1_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tv2PRosygHpO"
   },
   "source": [
    "## Final Function1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4VVLwmGDlAu"
   },
   "outputs": [],
   "source": [
    "def fun1(train):\n",
    "  train_features = get_train_features(train)\n",
    "  filename = '/content/drive/MyDrive/Colab Notebooks/CASE_STUDY_1/123'\n",
    "  lgbm = pd.read_pickle(filename)\n",
    "  predictions = lgbm.predict(train_features)\n",
    "  return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPtmwh3vgPOr"
   },
   "source": [
    "## Final Function2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KnCTu8KoD_eM"
   },
   "outputs": [],
   "source": [
    "def fun2(train,target):\n",
    "  predictions = fun1(train)\n",
    "  print('predicted score is : {}'.format(predictions))\n",
    "  actual = target\n",
    "  rmse = mean_squared_error(predictions, actual)**0.5\n",
    "  print('rmse value for is : {}'.format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ee3n1sUOQaDo",
    "outputId": "f763b699-001c-45c8-ad97-8e810e8253c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 4.04 MB\n"
     ]
    }
   ],
   "source": [
    "train = reduce_mem_usage(pd.read_csv('/content/train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z3Eoe9VnGBWP",
    "outputId": "1ac95814-ef83-42cf-b236-d7d87d93323a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data ........\n",
      "Memory usage after optimization is: 2.24 MB\n",
      "Memory usage after optimization is: 1749.11 MB\n",
      "Memory usage after optimization is: 114.20 MB\n",
      "processing train......\n",
      "processing historical transactions.....\n",
      "Memory usage after optimization is: 1638.06 MB\n",
      "processing new merchant transactions.....\n",
      "Memory usage after optimization is: 110.45 MB\n",
      "merging all files together......\n",
      "creating some more new features from merged file......\n",
      "preprocessing final data........\n",
      "rmse value for is : 3.415138445872954\n",
      "CPU times: user 16min 3s, sys: 20.5 s, total: 16min 24s\n",
      "Wall time: 15min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fun2(train,train['target'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Elo Feature Engineering.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
